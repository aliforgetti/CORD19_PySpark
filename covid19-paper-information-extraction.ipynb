{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# COVID 19 PySpark Latent Dirchlet Allocation \n## Objective\nThis notebook is to document the task of extracting topics from COVID19 Related Papers from the CORD-19 Dataset. This topic distribution will be used to help with extraction of similar papers that can help researchers conduct literature review and facilitate the processs of scientific research.\n\nIt is better to run this notebook on Kaggle as you will not have to download the dataset on your local computer.","metadata":{}},{"cell_type":"code","source":"# install relevant libraries\n!pip install langdetect\n!pip install pyspark\n!pip install sparknlp","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: langdetect in /opt/conda/lib/python3.7/site-packages (1.0.8)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from langdetect) (1.15.0)\nRequirement already satisfied: pyspark in /opt/conda/lib/python3.7/site-packages (3.1.1)\nRequirement already satisfied: py4j==0.10.9 in /opt/conda/lib/python3.7/site-packages (from pyspark) (0.10.9)\nRequirement already satisfied: sparknlp in /opt/conda/lib/python3.7/site-packages (1.0.0)\nRequirement already satisfied: spark-nlp in /opt/conda/lib/python3.7/site-packages (from sparknlp) (3.0.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sparknlp) (1.19.5)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport glob\nfrom time import time\nfrom tqdm import tqdm\n\nimport random\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport pyLDAvis\nfrom IPython.display import FileLink\n\nfrom nltk.corpus import stopwords\nfrom langdetect import detect\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml.feature import CountVectorizer, Tokenizer, StopWordsRemover,IDF\nfrom pyspark.ml.clustering import LDA\nfrom pyspark.ml import Pipeline\n\nfrom sparknlp.base import DocumentAssembler, Finisher\nfrom sparknlp.annotator import *\n\nimport glob\nfrom pathlib import Path","metadata":{"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"stop_words = stopwords.words('english')","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def init_spark():\n\n    spark = SparkSession.builder \\\n        .appName(\"Spark NLP\")\\\n        .master(\"local[4]\")\\\n        .config(\"spark.driver.memory\",\"60G\")\\\n        .config('spark.executor,memory','60G')\\\n        .config('spark.memory.offHeap.enabled',True)\\\n        .config('spark.memory.offHeap.size','100G')\\\n        .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.1\")\\\n        .getOrCreate()\n    \n    return spark","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"spark = init_spark()\nspark","metadata":{"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<pyspark.sql.session.SparkSession at 0x7fd5e1a5edd0>","text/html":"\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://d132f561b834:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[4]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Spark NLP</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "},"metadata":{}}]},{"cell_type":"code","source":"# do not run this if you want to conserve RAM space\n\nroot_path = Path('/kaggle/input/CORD-19-research-challenge/') # path to dataset on Kaggle\nmetadata_path = root_path / Path('metadata.csv') # path to metadata giving information about each paper\nmetadata = pd.read_csv(metadata_path, dtype={ 'pubmed_id': str, 'doi': str})\nmetadata.rename(columns={'source_x': 'source', 'WHO #Covidence': 'who_covidence'}, inplace=True)\nprint(\"There are \", len(metadata), \" sources in the metadata file.\")\n\nmetadata.head(2)","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"There are  529651  sources in the metadata file.\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   cord_uid                                       sha source  \\\n0  ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb    PMC   \n1  02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d    PMC   \n\n                                               title                    doi  \\\n0  Clinical features of culture-proven Mycoplasma...  10.1186/1471-2334-1-6   \n1  Nitric oxide: a pro-inflammatory mediator in l...           10.1186/rr14   \n\n      pmcid pubmed_id license  \\\n0  PMC35282  11472636   no-cc   \n1  PMC59543  11667967   no-cc   \n\n                                            abstract publish_time  \\\n0  OBJECTIVE: This retrospective chart review des...   2001-07-04   \n1  Inflammatory diseases of the respiratory tract...   2000-08-15   \n\n                                             authors         journal  mag_id  \\\n0                Madani, Tariq A; Al-Ghamdi, Aisha A  BMC Infect Dis     NaN   \n1  Vliet, Albert van der; Eiserich, Jason P; Cros...      Respir Res     NaN   \n\n  who_covidence_id arxiv_id  \\\n0              NaN      NaN   \n1              NaN      NaN   \n\n                                      pdf_json_files  \\\n0  document_parses/pdf_json/d1aafb70c066a2068b027...   \n1  document_parses/pdf_json/6b0567729c2143a66d737...   \n\n                               pmc_json_files  \\\n0  document_parses/pmc_json/PMC35282.xml.json   \n1  document_parses/pmc_json/PMC59543.xml.json   \n\n                                                 url  s2_id  \n0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...    NaN  \n1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cord_uid</th>\n      <th>sha</th>\n      <th>source</th>\n      <th>title</th>\n      <th>doi</th>\n      <th>pmcid</th>\n      <th>pubmed_id</th>\n      <th>license</th>\n      <th>abstract</th>\n      <th>publish_time</th>\n      <th>authors</th>\n      <th>journal</th>\n      <th>mag_id</th>\n      <th>who_covidence_id</th>\n      <th>arxiv_id</th>\n      <th>pdf_json_files</th>\n      <th>pmc_json_files</th>\n      <th>url</th>\n      <th>s2_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ug7v899j</td>\n      <td>d1aafb70c066a2068b02786f8929fd9c900897fb</td>\n      <td>PMC</td>\n      <td>Clinical features of culture-proven Mycoplasma...</td>\n      <td>10.1186/1471-2334-1-6</td>\n      <td>PMC35282</td>\n      <td>11472636</td>\n      <td>no-cc</td>\n      <td>OBJECTIVE: This retrospective chart review des...</td>\n      <td>2001-07-04</td>\n      <td>Madani, Tariq A; Al-Ghamdi, Aisha A</td>\n      <td>BMC Infect Dis</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>document_parses/pdf_json/d1aafb70c066a2068b027...</td>\n      <td>document_parses/pmc_json/PMC35282.xml.json</td>\n      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>02tnwd4m</td>\n      <td>6b0567729c2143a66d737eb0a2f63f2dce2e5a7d</td>\n      <td>PMC</td>\n      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n      <td>10.1186/rr14</td>\n      <td>PMC59543</td>\n      <td>11667967</td>\n      <td>no-cc</td>\n      <td>Inflammatory diseases of the respiratory tract...</td>\n      <td>2000-08-15</td>\n      <td>Vliet, Albert van der; Eiserich, Jason P; Cros...</td>\n      <td>Respir Res</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>document_parses/pdf_json/6b0567729c2143a66d737...</td>\n      <td>document_parses/pmc_json/PMC59543.xml.json</td>\n      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Metadata Description\nThis metadata allows us to get information about each paper. It consists of papers from the sources given below. It also contain the doi, title, abstract and other unique identification parameters that help us identify a given paper. ","metadata":{}},{"cell_type":"code","source":"metadata.source.value_counts()","metadata":{"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"WHO                                    218559\nMedline                                101433\nMedline; PMC                            86649\nPMC                                     47288\nElsevier; Medline; PMC                  42666\nMedRxiv; WHO                            10334\nElsevier; PMC                            6874\nArXiv                                    6284\nBioRxiv; WHO                             2411\nBioRxiv                                  1427\nMedline; PMC; WHO                         930\nMedRxiv                                   785\nMedRxiv; Medline; PMC; WHO                620\nElsevier                                  476\nBioRxiv; Medline; PMC; WHO                436\nMedRxiv; Medline; WHO                     349\nElsevier; Medline; PMC; WHO               305\nBioRxiv; Medline; WHO                     284\nElsevier; Medline                         255\nBioRxiv; MedRxiv                          248\nArXiv; Medline; PMC                       245\nMedline; WHO                              220\nArXiv; Medline                            200\nArXiv; Elsevier; Medline; PMC             124\nBioRxiv; MedRxiv; WHO                      73\nBioRxiv; Medline; PMC                      36\nPMC; WHO                                   30\nBioRxiv; Medline                           21\nMedRxiv; Medline; PMC                      20\nArXiv; PMC                                 12\nMedRxiv; Medline                           11\nArXiv; Elsevier; PMC                       11\nBioRxiv; MedRxiv; Medline; WHO              9\nArXiv; Elsevier                             5\nElsevier; PMC; WHO                          5\nBioRxiv; MedRxiv; Medline; PMC              4\nBioRxiv; MedRxiv; Medline; PMC; WHO         4\nArXiv; Elsevier; Medline; PMC; WHO          3\nArXiv; Elsevier; Medline                    2\nArXiv; MedRxiv; WHO                         2\nBioRxiv; MedRxiv; Medline                   1\nName: source, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## Extraction: Reading in COVID 19 Paper Data\nBelow are the functions we used to extract the data from the json dataset. Due to the limitations of the system we are operating on, we cannot use the full dataset and will have to limit the papers we operate on. This project serves as a proof of concept that the tasks described below could be used for the intended aim.\n\n**To simplify the task:**\n1. We are going to only use the abstracts instead of full text of the articles.\n2. We will also try and use the abstracts from the metadata itself. \n3. We will also limit the number of documents we will use. \n","metadata":{}},{"cell_type":"code","source":"sqlContext = SQLContext(spark)\n\n# Utility functions\ndef equivalent_type(f):\n    if f == 'datetime64[ns]': return TimestampType()\n    elif f == 'int64': return LongType()\n    elif f == 'int32': return IntegerType()\n    elif f == 'float64': return FloatType()\n    else: return StringType()\n\ndef define_structure(string, format_type):\n    try: typo = equivalent_type(format_type)\n    except: typo = StringType()\n    return StructField(string, typo)\n\n# Given pandas dataframe, it will return a spark's dataframe.\ndef pandas_to_spark(pandas_df):\n    columns = list(pandas_df.columns)\n    types = list(pandas_df.dtypes)\n    struct_list = []\n    for column, typo in zip(columns, types): \n      struct_list.append(define_structure(column, typo))\n    p_schema = StructType(struct_list)\n    return sqlContext.createDataFrame(pandas_df, p_schema)\n\ndef detect_lang(txt):\n    try:\n        return detect(txt)\n    except:\n        return None\n\ndef concatenate_text(j):\n    txt_all = \"\"\n    for a in j:\n        txt_all = txt_all + \" \" + a['text']\n    return txt_all\n\nudf_detect_lang = udf(detect_lang)\nudf_concatenate_text = udf(concatenate_text)","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def extract_paper_data(paper_path = \"../input/CORD-19-research-challenge/document_parses/pdf_json\",\n                    N = 1000, all_data = False):\n    \n    paper_jsons = []\n    count = 0\n    for file in os.listdir(paper_path):\n        if file.endswith(\".json\"):\n            paper_jsons.append(os.path.join(paper_path, file))\n            count+=1\n    \n    if all_data == False:\n        paper_sample = random.sample(paper_jsons, N) # randomly sample N papers from the dataframe. \n    else:\n        paper_sample = paper_jsons\n        \n    papers = spark.read\\\n                .format(\"json\")\\\n                .option(\"multiLine\", \"true\")\\\n                .load(paper_sample)\n\n    \n    papers = papers.select(papers['Paper_ID'], papers['metadata']['title'], udf_concatenate_text(papers['abstract']), udf_concatenate_text(papers['body_text']))\n\n    papers = papers.withColumnRenamed('metadata.title', 'title')\\\n                   .withColumnRenamed('concatenate_text(abstract)', 'abstract')\\\n                   .withColumnRenamed('concatenate_text(body_text)', 'body')\n    \n    # detect the language used in these papers\n    papers = papers.withColumn('lang', udf_detect_lang(papers['body']))\n    \n    \n    # filter out abstracts that\n    papers = papers.filter(papers['lang'] == 'en').filter(\"abstract != ''\").filter(\"title != ''\")\n    \n    return papers\n\n\ndef extract_metadata(metadata_path = \"../input/CORD-19-research-challenge/metadata.csv\",\n                    N = 1000, all_data = False):\n    \n    print(\"Extracting paper metadata...\")\n    meta_pd = pd.read_csv(metadata_path)\n    meta_pd = meta_pd[~meta_pd['abstract'].isnull()]\n    metadata = pandas_to_spark(meta_pd.head(N))\n\n    print(\"Detecting paper language...\")\n    # add column to give the language of the paper\n    metadata = metadata.withColumn('lang', udf_detect_lang(metadata['abstract']))\n\n    print(\"Filtering by selected language...\")\n    # filter to keep only papers with english language\n    metadata = metadata.filter(metadata['lang']=='en')\n\n    print(\"Returning DataFrame...\")\n\n    return metadata","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# example of what this looks like\nN = 500\n\n# paper_path = \"../input/CORD-19-research-challenge/document_parses/pdf_json\"\n# paper_df = extract_paper_data(paper_path, N)\n\npaper_df = extract_metadata(N = N)","metadata":{"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Extracting paper metadata...\nDetecting paper language...\nFiltering by selected language...\nReturning DataFrame...\n","output_type":"stream"}]},{"cell_type":"code","source":"print((paper_df.count(), len(paper_df.columns)))","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(500, 20)\n","output_type":"stream"}]},{"cell_type":"code","source":"paper_df.columns","metadata":{"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['cord_uid',\n 'sha',\n 'source_x',\n 'title',\n 'doi',\n 'pmcid',\n 'pubmed_id',\n 'license',\n 'abstract',\n 'publish_time',\n 'authors',\n 'journal',\n 'mag_id',\n 'who_covidence_id',\n 'arxiv_id',\n 'pdf_json_files',\n 'pmc_json_files',\n 'url',\n 's2_id',\n 'lang']"},"metadata":{}}]},{"cell_type":"markdown","source":"## NLP Pipeline","metadata":{}},{"cell_type":"code","source":"documentAssembler = DocumentAssembler()\\\n                    .setInputCol('abstract')\\\n                    .setOutputCol('document')\n\ntokenizer = Tokenizer()\\\n                    .setMinLength(4)\\\n                    .setInputCols(['document'])\\\n                    .setOutputCol('tokenized') # Identifies tokens with tokenization open standards\n    \nnormalizer = Normalizer()\\\n                    .setInputCols(['tokenized'])\\\n                    .setOutputCol('normalized')\\\n                    .setLowercase(True) # Removes all dirty characters from text\n\nlemmatizer = LemmatizerModel\\\n                    .pretrained()\\\n                    .setInputCols(['normalized'])\\\n                    .setOutputCol('lemmatized') # Retrieves lemmas out of words with the objective of returning a base dictionary word\n\nstopwords_cleaner = StopWordsCleaner()\\\n                    .setInputCols(['lemmatized'])\\\n                    .setOutputCol('unigrams')\\\n                    .setStopWords(stop_words) # removes stopwords\n\nngrammer = NGramGenerator()\\\n                    .setInputCols(['lemmatized'])\\\n                    .setOutputCol('ngrams')\\\n                    .setN(3)\\\n                    .setEnableCumulative(True)\\\n                    .setDelimiter('_') # gets ngrams\n\npos_tagger = PerceptronModel\\\n                    .pretrained('pos_anc')\\\n                    .setInputCols(['document', 'lemmatized'])\\\n                    .setOutputCol('pos') # sets POS tag to each word\n\nfinisher = Finisher().setInputCols(['unigrams', 'ngrams', 'pos'])\n\npipeline = Pipeline()\\\n            .setStages([documentAssembler,\\\n                        tokenizer,\\\n                        normalizer,\\\n                        lemmatizer,\\\n                        stopwords_cleaner,\\\n                        pos_tagger,\\\n                        ngrammer,\\\n                        finisher])\n\nfin_doc = pipeline.fit(paper_df).transform(paper_df)\n\nfin_doc.columns","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[OK!]\npos_anc download started this may take some time.\nApproximate size to download 3.9 MB\n[OK!]\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['cord_uid',\n 'sha',\n 'source_x',\n 'title',\n 'doi',\n 'pmcid',\n 'pubmed_id',\n 'license',\n 'abstract',\n 'publish_time',\n 'authors',\n 'journal',\n 'mag_id',\n 'who_covidence_id',\n 'arxiv_id',\n 'pdf_json_files',\n 'pmc_json_files',\n 'url',\n 's2_id',\n 'lang',\n 'finished_unigrams',\n 'finished_ngrams',\n 'finished_pos']"},"metadata":{}}]},{"cell_type":"code","source":"# initial pipeline\npos_documentAssembler = DocumentAssembler()\\\n                    .setInputCol('finished_pos')\\\n                    .setOutputCol('pos_document')\n\npos_tokenizer = Tokenizer()\\\n                    .setInputCols(['pos_document'])\\\n                    .setOutputCol('pos')\n\n# create column that gets the bigram POS combination\npos_ngrammer = NGramGenerator()\\\n                    .setInputCols(['pos'])\\\n                    .setOutputCol('pos_ngrams')\\\n                    .setN(3)\\\n                    .setEnableCumulative(True)\\\n                    .setDelimiter('_')\n\npos_finisher = Finisher()\\\n                    .setInputCols(['pos', 'pos_ngrams'])\n\npos_pipeline = Pipeline()\\\n                    .setStages([pos_documentAssembler,\\\n                                pos_tokenizer,\\\n                                pos_ngrammer,\\\n                                pos_finisher])\n\n# create a column which a collection of POS tags as they appear in the text of the abstract\nudf_join_arr = udf(lambda x: ' '.join(x), StringType())\nfin_doc  = fin_doc.withColumn('finished_pos', udf_join_arr(col('finished_pos')))\nfin_doc = pos_pipeline.fit(fin_doc).transform(fin_doc)\nfin_doc.columns","metadata":{"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"['cord_uid',\n 'sha',\n 'source_x',\n 'title',\n 'doi',\n 'pmcid',\n 'pubmed_id',\n 'license',\n 'abstract',\n 'publish_time',\n 'authors',\n 'journal',\n 'mag_id',\n 'who_covidence_id',\n 'arxiv_id',\n 'pdf_json_files',\n 'pmc_json_files',\n 'url',\n 's2_id',\n 'lang',\n 'finished_unigrams',\n 'finished_ngrams',\n 'finished_pos',\n 'finished_pos_ngrams']"},"metadata":{}}]},{"cell_type":"code","source":"def filter_pos(words, pos_tags):\n    res_words = []\n    for word, pos in zip(words, pos_tags):\n        if pos in  ['JJ', 'NN', 'NNS', 'VB', 'VBP']:\n            res_words.append(word)\n            \n    return res_words\n\ndef filter_pos_combs(words, pos_tags):\n    return [word for word, pos in zip(words, pos_tags) \n            if (len(pos.split('_')) == 2 and \\\n                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n            or (len(pos.split('_')) == 3 and \\\n                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n                  pos.split('_')[2] in ['NN', 'NNS'])]\n\nudf_filter_pos = udf(filter_pos, ArrayType(StringType()))\nudf_filter_pos_combs = udf(filter_pos_combs, ArrayType(StringType()))","metadata":{"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"fin_doc = fin_doc.withColumn('filtered_unigrams', udf_filter_pos(col('finished_unigrams'), col('finished_pos')))\nfin_doc = fin_doc.withColumn('filtered_ngrams', udf_filter_pos_combs(col('finished_ngrams'), col('finished_pos_ngrams')))\nfin_doc = fin_doc.withColumn('final', concat(col('filtered_unigrams'), col('filtered_ngrams')))","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"TF_vectorizer = CountVectorizer(inputCol='final', outputCol='tf_features')\nTF_mod = TF_vectorizer.fit(fin_doc)\nTF_result = TF_mod.transform(fin_doc)\n\nIDF_vectorizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\nIDF_mod = IDF_vectorizer.fit(TF_result)\nTFIDF_result = IDF_mod.transform(TF_result)","metadata":{"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## LDA : Topic Modeling","metadata":{}},{"cell_type":"code","source":"log_perplexity = []\nfor i in tqdm(range(5,25,5)):\n    num_topics = i\n    max_iter = 10\n    lda = LDA(k=num_topics, maxIter=max_iter, featuresCol= 'tf_idf_features')\n    lda_model = lda.fit(TFIDF_result)\n    log_perplexity.append(lda_model.logPerplexity(TFIDF_result))","metadata":{"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"100%|██████████| 4/4 [03:22<00:00, 50.53s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot([5, 10, 15, 20], np.exp(log_perplexity))\nplt.xlabel(\"Number of topics\")\nplt.ylabel(\"perplexity\")","metadata":{"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"Text(0, 0.5, 'perplexity')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAYIAAAERCAYAAAB2CKBkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8+yak3AAAACXBIWXMAAAsTAAALEwEAmpwYAAAlPUlEQVR4nO3deXgV9dn/8fdNWGWHBNlFdhBBFHG3rlXqXrQ/q0+t1tbWR21tlboWre1j3Wo3fbRWW2vrY6uyu+FSXKpWDVsg7CICYQv7EhKy3L8/Zo7EkJADZDLn5Hxe15UrOXMm53wIydwz98z3O+buiIhI5moUdwAREYmXCoGISIZTIRARyXAqBCIiGU6FQEQkw6kQiIhkuLQsBGb2ZzNbZ2Zzk1z/G2Y2z8zyzez/os4nIpJOLB3HEZjZycB24Bl3H1LLuv2A54HT3H2TmXVy93X1kVNEJB2k5RGBu78LbKy8zMz6mNlrZjbdzN4zs4HhU98DHnX3TeH3qgiIiFSSloWgBk8AN7j7UcDNwP+Gy/sD/c3sfTP7j5mdHVtCEZEU1DjuAHXBzFoBxwMvmFlicbPwc2OgH3AK0B1418wOd/fN9RxTRCQlNYhCQHBks9ndj6jmuZXAR+5eCnxmZosICsMn9ZhPRCRlNYjWkLtvJdjIXwJggWHh0xMJjgYws2yCVtHSGGKKiKSktCwEZvYc8CEwwMxWmtnVwOXA1WY2G8gHLghXnwpsMLN5wDRgjLtviCO3iEgqSsvLR0VEpO6k5RGBiIjUnbQ7WZydne29evWKO4aISFqZPn36enfPqe65tCsEvXr1Ijc3N+4YIiJpxcw+r+k5tYZERDKcCoGISIZTIRARyXAqBCIiGU6FQEQkw6kQiIhkOBUCEZEMp0IgIpIGfvvmIuas3BLJa6fdgDIRkUzz5ry1/PbNxZSVO4d3b1vnr68jAhGRFLa5aBe3TZjDwM6tueH0vpG8h44IRERS2M+nzGPjjl385cqjadY4K5L30BGBiEiKej1/DRNmFnDdqX0Z0q3uW0IJKgQiIilo045d3D5hLoO6tOH6U6NpCSWoNSQikoLunpLP5qJd/PU7R9O0cbT77DoiEBFJMa/NXcOkWau4/rS+HNY1upZQggqBiEgK2bhjF3dOnMPgLm24LuKWUIJaQyIiKWTspLls2VnKM985hiZZ9bOvriMCEZEU8cqc1byUt5obTuvH4K5t6u19VQhERFLAhu0l/GziXIZ0a8O1p/Sp1/dWa0hEJAWMnZTP1uJSnr2k/lpCCToiEBGJ2Ut5q3h5zmpuPKM/AzvXX0soQYVARCRG67eXMHZSPkO7t+X7J/eOJYMKgYhITNydn02cy/biMh66ZBiN67kllKBCICISkyl5q3l17hpuPLMf/Q9uHVsOFQIRkRis21bM2ElzGdajHdecFE9LKEGFQESknrk7d06YS9Guch66eGhsLaGEyN7dzHqY2TQzm2dm+Wb2o2rWOcXMtpjZrPBjbFR5RERSxeTZq3h93lp+cmZ/+sXYEkqIchxBGXCTu88ws9bAdDN7w93nVVnvPXc/N8IcIiIpY93WYsZOymd4z3Z8L+aWUEJkRwTuvtrdZ4RfbwPmA92iej8RkVTn7tw+YS47S8t58OJhZDWyuCMB9XSOwMx6AcOBj6p5+jgzm21mr5rZYTV8/zVmlmtmuYWFhVFGFRGJzMRZBbw5fy1jvjqAvp1axR3nC5EXAjNrBYwDbnT3rVWengEc4u7DgD8AE6t7DXd/wt1HuPuInJycSPOKiERh7dZi7pqUz1GHtOc7Jx4ad5wvibQQmFkTgiLwrLuPr/q8u2919+3h168ATcwsO8pMIiL1zd25ffwcSsoqePDioSnTEkqI8qohA54C5rv7wzWs0zlcDzMbGebZEFUmEZE4jJtRwFsL1jHmrAH0zkmdllBClFcNnQB8C5hjZrPCZbcDPQHc/XHgYuBaMysDdgKXurtHmElEpF6t2VLMz6fkc3Sv9lx1Qmq1hBIiKwTu/m9gr8c/7v4I8EhUGURE4uTu3Do+j9LyipS6SqgqjSwWEYnIC9NX8vbCQn561kB6ZbeMO06NVAhERCKwavNOfjFlHiN7deDK43vFHWevVAhEROpY0BKaQ1mF8+AlQ2mUoi2hBBUCEZE69nzuCt5dVMitowZySMfUbQklqBCIiNShgs07+cVL8zm2dwe+dewhccdJigqBiEgdcXduHZdHhTsPjB6W8i2hBBUCEZE68tzHK3hv8XpuGzWQnh0PijtO0lQIRETqwMpNRfzPy/M4vk9HLj8mPVpCCSoEIiIHyN25ZVweAPePTv2rhKpSIRAROUDPfrSc95ds4LavDaJHh/RpCSWoEIiIHIAVG4u495X5nNg3m8uP6Rl3nP2iQiAisp8qKpyfvphHIzPuG3044WTKaUeFQERkPz370ed8uHQDd5wziO7t068llKBCICKyH5ZvKOLeVxZwUr9sLj26R9xxDogKgYjIPqqocMa8OJvGjYz7Rw9N25ZQggqBiMg+eubDZXz02UbuPHcQXdu1iDvOAVMhEBHZB8vW7+D+1xbylf45fGNEereEElQIRESSlLhKqHFWel8lVJUKgYhIkp7+YBkfL9vI2HMH06Vt+reEElQIRESS8Nn6HTwwdQGnDezExUd1jztOnVIhEBGpRXmFM+aF2TTJasS9FzWcllBC47gDiIikur+8/xm5n2/i15cMo3Pb5nHHqXM6IhAR2YtPC7fz4NSFnD6wE18/slvccSKhQiAiUoNES6h5kyzu/XrDawklqDUkIlKDp/69lBnLN/Ob/zeMg9s0vJZQgo4IRESqsWTddh56fRFnDj6YC49omC2hBBUCEZEqyiucm1+YzUFNs/ifi4Y02JZQglpDIiJV/Om9pcxasZnfXXoEnVo33JZQQmRHBGbWw8ymmdk8M8s3sx9Vs46Z2e/NbImZ5ZnZkVHlERFJxuK123j49UWcddjBnD+sa9xx6kWURwRlwE3uPsPMWgPTzewNd59XaZ1RQL/w4xjgsfCziEi9Kyuv4OYXZtOyWRa/vLDhXiVUVWRHBO6+2t1nhF9vA+YDVc+4XAA844H/AO3MrEtUmURE9uaJ95Yye+UW7rlgCDmtm8Udp97Uy8liM+sFDAc+qvJUN2BFpccr2bNYYGbXmFmumeUWFhZGllNEMtfCNdv47RuLGTWkM+cOzaz90cgLgZm1AsYBN7r71v15DXd/wt1HuPuInJycug0oIhmvNGwJtWremF9c2PCvEqoq0quGzKwJQRF41t3HV7NKAVD5zg7dw2UiIvXmj+98ypyCLTx62ZFkt8qcllBClFcNGfAUMN/dH65htcnAFeHVQ8cCW9x9dVSZRESqWrBmK797azHnDO3CORnWEkqI8ojgBOBbwBwzmxUuux3oCeDujwOvAF8DlgBFwFUR5hER+ZLS8gpuen42bZo34Z7zD4s7TmwiKwTu/m9gr402d3fguqgyiIjszWNvf0r+qq08dvmRdMzAllCCppgQkYw0b9VW/vCvxZw3rCujDs/MllCCCoGIZJzEVUJtWzTN6JZQguYaEpGM8+i0JcxbvZU/fuso2rdsGnec2OmIQEQySv6qLTzyryVccERXzjqsc9xxUoIKgYhkjF1lwVVC7Vs25e7z1BJKUGtIRDLGI/9azII12/jTFSPUEqpERwQikhHmFmzh0bc/5evDu3Hm4IPjjpNSVAhEpMErKSvnpudn07FlU+5SS2gPag2JSIP3h7eWsHDtNp769gjaHtQk7jgpR0cEItKg5a3czGPvfMroI7tz+iC1hKqjQiAiDVZJWTk3vzCb7FZNGXve4LjjpCy1hkSkwfrdm4tZtHY7f7nqaNq2UEuoJjoiEJEGadaKzTz+zqd8Y0R3Th3QKe44KS2pQmBm483sHDNT4RCRlFdcGrSEDm7TnDvPVUuoNslu2P8XuAxYbGb3mdmACDOJiByQ37y5iCXrtnPf6KG0aa6WUG2SKgTu/qa7Xw4cCSwD3jSzD8zsqvB2lCIiKWHG8k386d2lXHp0D77SX/c4T0bSrR4z6whcCXwXmAn8jqAwvBFJMhGRfZRoCXVu05w7zhkUd5y0kdRVQ2Y2ARgA/A04r9J9hf9pZrlRhRMR2RcPv7GIpYU7+NvVI2mtllDSkr189E/u/krlBWbWzN1L3H1EBLlERPbJ9M838qf3lvLNkT05qZ9aQvsi2dbQL6tZ9mFdBhER2V/FpeWMeSGPrm1bqCW0H/Z6RGBmnYFuQAszG87um9G3AQ6KOJuISFIemrqQpet38Ox3j6FVM42T3Ve1/cTOIjhB3B14uNLybcDtEWUSEUnaJ8s28tT7n/Ffx/bkhL7ZccdJS3stBO7+V+CvZjba3cfVUyYRkaTs3FXOmBdm061dC24bpZbQ/qqtNfRf7v53oJeZ/aTq8+7+cDXfJiJSLx6YuoBlG4r4v+8dQ0u1hPZbbT+5luHnVlEHERHZFx8t3cDTHyzjiuMO4fg+agkdiNpaQ38MP/+86nNmpht+ikgsinaVMebFPLq3b8EtZw+MO07aS3bSubfNrFelx0cDn0QVSkRkbx54bSHLNxbx4MXD1BKqA8n+BH8FvGZmvye4nHQUcFVkqUREavCfsCV05fG9OLZ3x7jjNAhJFQJ3n2pmPyCYV2g9MNzd1+zte8zsz8C5wDp3H1LN86cAk4DPwkXj3f2e5KOLSKbZUVLGmBdnc0jHg/jp2ZoEua4kO9fQz4BvACcDQ4G3zewmd395L9/2NPAI8Mxe1nnP3c9NMquIZLj7X1vAyk07+ec1x3FQU7WE6kqyP8mOwEh33wl8aGavAU8CNRYCd3+38nkFEZED8cGS9Tzz4ed854RDGXloh7jjNCjJ3o/gRoDEDWnc/XN3P7MO3v84M5ttZq+a2WE1rWRm15hZrpnlFhYW1sHbikg62V5Sxk/H5XFodkvGnKWWUF1L9qqh84BZwGvh4yPMbPIBvvcM4BB3Hwb8AZhY04ru/oS7j3D3ETk5mlVQJNP86pX5FGzeyYMXD6VF06y44zQ4yc4+ejcwEtgM4O6zgN4H8sbuvtXdt4dfvwI0MTONChGRL/n34vU8+9Fyrj7hUEb0UksoCskWglJ331JlWcWBvLGZdTYzC78eGWbZcCCvKSINy7biUm4Zl0fv7JbcrJZQZJI9WZxvZpcBWWbWD/gh8MHevsHMngNOAbLNbCVwF9AEwN0fBy4GrjWzMmAncKm7+379K0SkQbr3lQWs3rKTF35wPM2bqCUUlWQLwQ3AHUAJ8BwwFfjF3r7B3b9Zy/OPEFxeKiKyh3cXFfLcx8u55uTeHHVI+7jjNGjJDigrIigEd0QbR0QEthaXcuu4PPrktOQnZ/aPO06DV9s01FOAGts17n5+nScSkYx378vzWbO1mHHXqiVUH2o7InioXlKIiITeXriOf3yygh98pQ/De6olVB9qm4b6ncTX4bTTAwmOEBa6+66Is4lIhtmys5Rbx82hX6dW3HhGv7jjZIxk5xo6B3gc+JTgBvaHmtn33f3VKMOJSGb55UvzKNxewh+/dZRaQvUo2auGfg2c6u5LAMysD8E8QyoEIlInpi1YxwvTV/Lfp/RhWI92ccfJKMkOKNuWKAKhpcC2CPKISAbaUlTKrePz6H9wK36kllC9S/aIINfMXgGeJzhHcAnwiZl9HcDdx0eUT0QywD0vzWP99l08ecXRNGusllB9S7YQNAfWAl8JHxcCLYDzCAqDCoGI7Je35q9l3IyVXH9qXw7v3jbuOBmp1kJgZllAnrv/ph7yiEgG2VJUym3j5zCwc2tuOL1v3HEyVq3nCNy9HNjrdBEiIvvj51Py2bBjFw9dMkwtoRgl2xp638weAf4J7EgsdPcZkaQSkQbvjXlrGT+zgB+e3o8h3dQSilOyheCI8HPlm8s7cFqdphGRjLBpxy5unzCHQV3acP2pagnFLdlJ506NOoiIZI67p+Szaccunr7qaJo2TvYqdolKsreqPNjMnjKzV8PHg83s6mijiUhD9NrcNUyatYrrT+vLYV3VEkoFyZbipwnuQdA1fLwIuDGCPCLSgG3csYs7J85hcJc2XKeWUMpIthBku/vzhLendPcyoDyyVCLSIN01OZ8tO0v59TeG0SRLLaFUkez/xA4z60h4bwIzOxaoeg9jEZEavTpnNVNmr+KG0/oxqEubuONIJcleNfQTYDLQ28zeB3II7jksIlKrDdtLuHPiXIZ0a8O1p/SJO45UkWwhmAdMAIoIJpubSHCeQESkVmMn57O1uJRnLzlGLaEUlOz/yDMEN6W5F/gD0B/4W1ShRKTheDlvNS/nrebGM/ozsLNaQqko2SOCIe4+uNLjaWY2L4pAItJwrN9ews8mzWVo97Z8/+TecceRGiR7RDAjPEEMgJkdA+RGE0lEGgJ352cT57K9uIyHLhlGY7WEUlayRwRHAR+Y2fLwcU9goZnNAdzdh0aSTkTS1kt5q3l17hp+evYA+h/cOu44shfJFoKzI00hIg1K4bYSxk6ay7Ae7bjmJLWEUl2ycw19HnUQEWkY3J07J85hx65yfn3JULWE0oD+h0SkTk2evYqp+Wu56cz+9O2kllA6UCEQkTqzblsxd03OZ3jPdnxXLaG0EVkhMLM/m9k6M5tbw/NmZr83syVmlmdmR0aVRUSi5+7cMWEuRbvKefDiYWQ1srgjSZKiPCJ4mr2fZB4F9As/rgEeizCLiERs4qwC3pi3ljFfHUDfTq3ijiP7ILJC4O7vAhv3ssoFwDMe+A/Qzsy6RJVHRKKzdmsxd0+ex1GHtOc7Jx4adxzZR3GeI+gGrKj0eGW4bA9mdo2Z5ZpZbmFhYb2EE5HkuDu3j59DcWk5D148VC2hNJQWJ4vd/Ql3H+HuI3JycuKOIyKVjJ9RwFsL1jHmrAH0zlFLKB3FWQgKgB6VHncPl4lImlizpZi7p+RzdK/2XHWCWkLpKs5CMBm4Irx66Fhgi7uvjjGPiOwDd+e28XmUllfoKqE0l+wUE/vMzJ4DTgGyzWwlcBfQBMDdHwdeAb4GLCG4z8FVUWURkbr34vSVTFtYyF3nDaZXdsu448gBiKwQuPs3a3negeuien8Ric7qLTu5Z8o8Rh7agW8f1yvuOHKA0uJksYikDnfn1nFzKKtwHrx4KI3UEkp7KgQisk+ez13BO4sKuXXUQA7pqJZQQ6BCICJJK9i8k1++NJ9je3fgW8ceEnccqSMqBCKSlKAllEe5Ow+MHqaWUAOiQiAiSfnHJyt4b/F6bhs1kJ4dD4o7jtQhFQIRqdXKTUX88qV5HN+nI5cfo5ZQQ6NCICJ7lbhKCOD+0bpKqCFSIRCRvfq/j5fz7yXruf2cQfTooJZQQ6RCICI1WrGxiHtfns+JfbO5bGTPuONIRFQIRKRaFRXOLePyMDPuG304ZmoJNVQqBCJSrWc/+pwPPt3AHecMont7tYQaMhUCEdnD8g1F/OrVBZzUL5tLj+5R+zdIWlMhEJEvqahwxrw4mywz7h89VC2hDBDZ7KMikl427djFy3NWM27GSmYu38z9ow+na7sWcceSeqBCIJLBikvL+deCdUyYWcDbC9dRWu7069SKsecO5hsj1BLKFCoEIhmmosL5eNlGJs4s4OU5q9lWXEan1s248vheXDi8G4O7tFE7KMOoEIhkiEVrtzFhZgGTZhawaksxBzXN4uwhnbloeDeO75OtW01mMBUCkQZs3dZiJs9exfgZBcxbvZWsRsbJ/bK5ZdRAzhx8MAc11SZAVAhEGpztJWVMnbuGibMKeH/JeiochnVvy13nDebcoV3Jad0s7oiSYlQIRBqAsvIK3luynokzC5iav4bi0gp6dGjB9af25YLh3eiT0yruiJLCVAhE0pS7k7dyCxNmFvBS3irWb99F2xZNGH1kdy4a3o2jDmmvk76SFBUCkTSzYmMRE2cWMGFWAUsLd9C0cSPOGNSJC4/oxikDOtG0scaJyr5RIRBJA4nBXhNnFpD7+SYAjjm0A9ec1JtRh3ehbYsmMSeUdKZCIJKiikvLmbZgHeMrDfbq26kVY84awAVHdNVEcFJnVAhEUkh1g71yWjfj28cFg70O66rBXlL3VAhEUsDitdsYX3Ww12GduehIDfaS6KkQiMQkMdhrwswC8lcFg71O0mAviUGkv2lmdjbwOyALeNLd76vy/JXAg0BBuOgRd38yykwicdpRUsbU/DVMmLl7sNdQDfaSmEVWCMwsC3gUOBNYCXxiZpPdfV6VVf/p7tdHlUMkbpUHe72ev5adpeV0b9+C607tywVHdKNvJw32knhFeUQwElji7ksBzOwfwAVA1UIg0uC4O3MKtjB+xpcHe110ZDe+rsFekmKiLATdgBWVHq8EjqlmvdFmdjKwCPixu6+oZh2RtLDHYK+sRpw+qBMXDu/GKQNyaNY4K+6IInuI+2zUFOA5dy8xs+8DfwVOq7qSmV0DXAPQs2fP+k0oUovNRbt4Ke/Lg71GJgZ7DelC24M02EtSW5SFoACofIuj7uw+KQyAu2+o9PBJ4IHqXsjdnwCeABgxYoTXbUyRfZcY7DVhZgHTNNhL0lyUheAToJ+ZHUpQAC4FLqu8gpl1cffV4cPzgfkR5hE5IBrsJQ1VZIXA3cvM7HpgKsHlo39293wzuwfIdffJwA/N7HygDNgIXBlVHpH9tThxZ69ZqyjYvPOLwV4XDu/G8X060jhLk7xJejP39Oq0jBgxwnNzc+OOIQ1cTYO9LhreTYO9JC2Z2XR3H1Hdc/ptFgnVNNhr7LmDOW+YBntJw6VCIBlNg71EVAgkAyUGe02YWcCU2V8e7HXR8G4c1bM9jTTJm2QQFQLJGBrsJVI9FQJp0DYX7b6z1yfLdg/2+t5JvfmaBnuJACoE0gBpsJfIvlEhkAahosL5ZNlGJs4q4OW81WwtLiO7VTOuOK4XF2mwl8heqRBIWtNgL5EDp0IgaScx2GvirALmFmylkcFJ/XIYc9YAvnqYBnuJ7Cv9xUha2Ntgr3OHdaFT6+ZxRxRJWyoEkrLKyiv4dzjYa2o42Ktbuxb89yl9uXB4V/p2ah13RJEGQYVAUooGe4nUPxUCSQkrNhYxaVYBE2YW8Gk42Ou0gcFgr1MHarCXSJRUCKTeuDubi0pZt62Ewm0lFG4vZvWWYqYtWPelwV7f1WAvkXqlQiAHrLi0PNywBxv4Lzb020oo3Fb8xbL120soLd9z2vPEYK/zh3WlRwcN9hKpbyoEUi13Z1NR6Rcb9HWVNuhVl20tLtvj+82gY8um5LRuTk7rZvTt1JpObZqR06oZOa2b0al18DmndTNaN9eev0icVAgyTGLvfXd7poTCrcV77M3XtPfeoknWFxv0AZ1bc2Lf7HDD3vyLDXun1s3o0LKpBnOJpAkVggagosLZvLP0iz30mvbc9773vnsj3v/g1l/ea2/VjE5tgg19y6ZZmqpBpIFRIUhhe+y9Jzbo20tYt3X35/XbSyirqHnvvVPr3Xvvndo0D9oz4V699t5FRIWgnlVUOJuKdu1uxWz98oa9cFvxFxv+bXvZe0/srQ8I996ra8+0bKb/XhGpnbYUdWT33ntN7Zndvffq9t5bNs36YiM+qHMbTu63+2Tq7vZMMzocpL13EalbKgR7kdh7r26DHuzFhydZt5awrWTPvfdGBh1b7d6ID+xcuffe/EtX0WjvXUTikpFbn+LS8rAVU90lkbtPsK7fvovyGvbeE732qnvvlS+L7NiyGVmaDkFEUlzGFIJpC9fxiynzgt57DXvv2a12b8QHdWm9xxUz2nsXkYYoY7Zo7Vo0YVDXNpxcZUBT4gRrh5ZNtfcuIhkpYwrB8J7tefSy9nHHEBFJObr8REQkw6kQiIhkuEgLgZmdbWYLzWyJmd1azfPNzOyf4fMfmVmvKPOIiMieIisEZpYFPAqMAgYD3zSzwVVWuxrY5O59gd8A90eVR0REqhflEcFIYIm7L3X3XcA/gAuqrHMB8Nfw6xeB000zmomI1KsoC0E3YEWlxyvDZdWu4+5lwBagY9UXMrNrzCzXzHILCwsjiisikpnS4mSxuz/h7iPcfUROTk7ccUREGpQoC0EB0KPS4+7hsmrXMbPGQFtgQ4SZRESkiigHlH0C9DOzQwk2+JcCl1VZZzLwbeBD4GLgX+6+5+Q+lUyfPn29mX0eQd79lQ2sjzvEXqR6Pkj9jKmeD5SxLqR6PjiwjIfU9ERkhcDdy8zsemAqkAX82d3zzeweINfdJwNPAX8zsyXARoJiUdvrplRvyMxy3X1E3Dlqkur5IPUzpno+UMa6kOr5ILqMkU4x4e6vAK9UWTa20tfFwCVRZhARkb1Li5PFIiISHRWCA/dE3AFqker5IPUzpno+UMa6kOr5IKKMVsu5WRERaeB0RCAikuFUCEREMpwKwX4ys3Zm9qKZLTCz+WZ2XNyZqjKzH5tZvpnNNbPnzKx5CmT6s5mtM7O5lZZ1MLM3zGxx+Dm2OwjVkO/B8P85z8wmmFm7uPKFefbIWOm5m8zMzSw7jmxhhmrzmdkN4c8x38weiCtfmKW6/+cjzOw/ZjYrnNJmZIz5epjZNDObF/68fhQuj+RvRYVg//0OeM3dBwLDgPkx5/kSM+sG/BAY4e5DCMZy1DpOox48DZxdZdmtwFvu3g94K3wcl6fZM98bwBB3HwosAm6r71BVPM2eGTGzHsBXgeX1HaiKp6mSz8xOJZhkcpi7HwY8FEOuyp5mz5/hA8DP3f0IYGz4OC5lwE3uPhg4FrgunL05kr8VFYL9YGZtgZMJBsTh7rvcfXOsoarXGGgRTt9xELAq5jy4+7sEgwcrqzwL7V+BC+szU2XV5XP318NJEQH+QzBdSmxq+BlCMJX7T4FYrwCpId+1wH3uXhKus67eg1VSQ0YH2oRftyXGvxd3X+3uM8KvtxHsaHYjor8VFYL9cyhQCPzFzGaa2ZNm1jLuUJW5ewHBXtdyYDWwxd1fjzdVjQ5299Xh12uAg+MMU4vvAK/GHaIqM7sAKHD32XFnqUF/4KTwBlTvmNnRcQeqxo3Ag2a2guBvJ+4jPwDCG3YNBz4ior8VFYL90xg4EnjM3YcDO4i3nbGHsHd4AUHR6gq0NLP/ijdV7cK5plLymmYzu4PgkP3ZuLNUZmYHAbcTtDNSVWOgA0GbYwzwfAree+Ra4Mfu3gP4MeERf5zMrBUwDrjR3bdWfq4u/1ZUCPbPSmClu38UPn6RoDCkkjOAz9y90N1LgfHA8TFnqslaM+sCEH6OtW1QHTO7EjgXuLy2iRFj0Ieg4M82s2UErasZZtY51lRfthIY74GPgQqCCdRSybcJ/k4AXiC4uVZszKwJQRF41t0TuSL5W1Eh2A/uvgZYYWYDwkWnA/NijFSd5cCxZnZQuOd1Oil2QruSxCy0hJ8nxZhlD2Z2NkHv/Xx3L4o7T1XuPsfdO7l7L3fvRbDRPTL8PU0VE4FTAcysP9CU1JvpcxXwlfDr04DFcQUJ/2afAua7+8OVnormb8Xd9bEfH8ARQC6QR/BL3j7uTNVk/DmwAJgL/A1olgKZniM4Z1FKsMG6muCudG8R/OG9CXRIsXxLCO6kNyv8eDzVfoZVnl8GZKdSPoIN/9/D38UZwGmp9jMETgSmA7MJ+vFHxZjvRIK2T16l37uvRfW3oikmREQynFpDIiIZToVARCTDqRCIiGQ4FQIRkQynQiAikuFUCCQthDNq/rrS45vN7O46eu2nzeziunitWt7nknCm2mlVlvcys8sO8LU/OLB0kslUCCRdlABfj3N65eqEE/ol62rge+5+apXlvYADKgTunqqjxiUNqBBIuigjuF/rj6s+UXWP3sy2h59PCSc4m2RmS83sPjO73Mw+NrM5Ztan0sucEc5Bv8jMzg2/Pyu8F8En4b0Ivl/pdd8zs8lUM6LczL4Zvv5cM7s/XDaWYJDQU2b2YJVvuY9gQrZZFtxDormZ/SV8jZnhFM6Y2ZXhv+XtcD76u6r+m8Ovbwm/d7aZ3Rcu+2E4t32emf1jX37w0vDty96MSNweBfL28aYmw4BBBFMOLwWedPeR4Y0+biCYcRKCvfKRBPP2TDOzvsAVBLO2Hm1mzYD3zSwxg+uRBPco+Kzym5lZV+B+4ChgE/C6mV3o7veY2WnAze6eWyXjreHyRAG6iWBOscPNbGD4Gv3DdUcCQ4Ai4BMze7ny65nZKILJBo9x9yIz61DpPQ519xKL+cY6knp0RCBpw4PZF58huOFOsj7xYG73EuBTILEhn0Ow8U943t0r3H0xQcEYSHCTlyvMbBbBlAMdgX7h+h9XLQKho4G3PZjsLzFT6cn7kBeCI4e/A7j7AuBzgmmcAd5w9w3uvpNggrQTq3zvGcBfPJwTyd0Tc+7nAc+GM9CWIVKJCoGkm98S9Nor3/+hjPB32cwaEcxrk1BS6euKSo8r+PIRcdW5Vhww4AZ3PyL8ONR339Nhx4H8Iw5AdTmTcQ7BEdWRBEcS6gbIF1QIJK2Ee7jPExSDhGUErRiA84Em+/HSl5hZo/C8QW9gITAVuDacDhgz65/EDYg+Br5iZtlmlgV8E3inlu/ZBrSu9Pg94PLEewI9wzwAZ1pw39oWBHener/Ka70BXBXeoyBxj9tGQA93nwbcQnD3rVa1ZJIMor0CSUe/Bq6v9PhPwCQzmw28xv7trS8n2Ii3AX7g7sVm9iRB+2hGOC1wIbXcGtDdV5vZrcA0giOKl929tqmC84DyMP/TwP8Cj5nZHIKjnSvD3j5hxnEE9xz4e9XzDe7+mpkdAeSa2S7gFeAu4O8W3GLVgN97at5aVWKi2UdF0oQFN8cZ4e7X17auyL5Qa0hEJMPpiEBEJMPpiEBEJMOpEIiIZDgVAhGRDKdCICKS4VQIREQy3P8H9cf6lDwHI5MAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"We will keep the number of topics to 10 as the perplexity of the model is still low while keeping the number of topics minimal.","metadata":{}},{"cell_type":"code","source":"num_topics = 10\nmax_iter = 10\n\nlda = LDA(k=num_topics, maxIter=max_iter, featuresCol= 'tf_idf_features')\nlda_model = lda.fit(TFIDF_result)\nlda_result = lda_model.transform(TFIDF_result)","metadata":{"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"num_top_words = 10\n\nvocab = TF_mod.vocabulary # get vocabulary from the term frequency model\n\ndef get_words(token_list):\n     return [vocab[token_id] for token_id in token_list]\n       \nudf_to_words = udf(get_words, ArrayType(StringType()))\n\ntopics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(col('termIndices')))\ntopics.select('topic', 'topicWords').show(truncate=90)","metadata":{"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"+-----+------------------------------------------------------------------------------------------+\n|topic|                                                                                topicWords|\n+-----+------------------------------------------------------------------------------------------+\n|    0|[public, public_health, health, tabletop_exercise, antiviral_drug, health_department, g...|\n|    1|[chinese, apom, journal, chinese_journal, kong, hong, rs, bibliographic_database, publi...|\n|    2|[patient, influenza, care, pandemic, hn, mortality, health, intensive_care, infection, ...|\n|    3|[epidemic, distance, epiflex, metric, trace, reproduction_number, reproduction, road_di...|\n|    4|              [peptide, cell, tumor, nsclc, target_phage, target, fusion, np, mouse, bind]|\n|    5|[liver, pemfs, mesothelioma, pleural, hse, microtubule, bile, ecmo, reovirus, ecmo_trea...|\n|    6|[exacerbation, exacerbation_severity, surfactant, pseudoknot, frameshifting, pseudoknot...|\n|    7|[influenza_activity, season, influenza, spread, sentinel, week, activity, population, r...|\n|    8|[cox, drak, dynamin, entry, btv, apoptosis, virus_entry, activity, be_inhibit, infectio...|\n|    9|      [protein, virus, domain, ntail, cell, interaction, gene, sequence, structure, viral]|\n+-----+------------------------------------------------------------------------------------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Data Visualization - \n# def format_data_pyldavis(clean_df, cvmodel, lda_trans,lda_model):\n    \n#     counts = clean_df.select((explode(clean_df.select(\"unigram\"))).alias('tokens')).groupby('tokens').count()\n    \n#     wc = {i['tokens']:i['count'] for i in counts.collect()}\n    \n#     wc = [wc[x] for x in cvmodel.vocabulary]\n    \n#     data = {'topic_term_dists': np.array(lda_model.topicsMatrix().toArray()).T,\n#            'doc_topic_dists': np.array([x.toArray() for x in lda_trans.select(['topicDistribution']).toPandas()['topicDistribution']]),\n#            'doc_lengths': [x[0] for x in clean_df.select(size(clean_df.filtered)).collect()][:-1],\n#            'vocab': cvmodel.vocabulary,\n#            'term_frequency': wc}\n#     return data\n\n# data = format_data_pyldavis(lda_result, TF_mod, lda_result, lda_model)\n# py_lda_data = pyLDAvis.prepare(**data)\n# file_name = '/output/data-viz.html'\n# pyLDAvis.display(py_lda_data)","metadata":{"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## BERT Sentence Embeddings","metadata":{}},{"cell_type":"code","source":"documentAssembler = DocumentAssembler()\\\n                    .setInputCol('abstract')\\\n                    .setOutputCol('document')","metadata":{"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"doc_ass = documentAssembler.transform(lda_result)","metadata":{"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"bert = BertSentenceEmbeddings.pretrained() \\\n    .setInputCols(\"document\") \\\n    .setOutputCol(\"bert_sentence_embeddings\")","metadata":{"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"sent_small_bert_L2_768 download started this may take some time.\nApproximate size to download 139.6 MB\n[OK!]\n","output_type":"stream"}]},{"cell_type":"code","source":"bert_lda_df = bert.transform(doc_ass)","metadata":{"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"## Recommend Similar Papers\nWe are going to use the topic distributiosn of each paper to find the ones that are most similar through cosine similarity between the distributions.","metadata":{}},{"cell_type":"code","source":"# randomly sample. This will be replaced by a search methodology by researchers\nrandom_sample = bert_lda_df.sample(False, 0.1, seed=42)","metadata":{"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"static_vector_topic = random_sample.select(\"topicDistribution\").limit(1).collect()[0][0]\nstatic_vector_embedding = random_sample.select('bert_sentence_embeddings').limit(1).collect()[0][0][0][5]","metadata":{"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"random_sample.select(\"title\").first()[0]","metadata":{"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"'The 21st International Symposium on Intensive Care and Emergency Medicine, Brussels, Belgium, 20-23 March 2001'"},"metadata":{}}]},{"cell_type":"code","source":"def cos_sim(a,b):\n    return float(np.dot(a,b)/ (np.linalg.norm(a) * np.linalg.norm(b)))","metadata":{"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"cos_df = bert_lda_df\\\n    .withColumn(\"coSim_topic\", udf(cos_sim, FloatType())(col('topicDistribution'),\\\n                                                   array([lit(v) for v in static_vector_topic])))\n\ncos_df = cos_df\\\n    .withColumn(\"coSim_bert\", udf(cos_sim, FloatType())(col('bert_sentence_embeddings'),\\\n                                                   array([lit(v) for v in static_vector_embedding])))","metadata":{"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"cos_df= cos_df.withColumn(\"coSim_sum\", col(\"coSim_topic\")+col(\"coSim_bert\"))","metadata":{"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"cos_df.columns","metadata":{"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"['cord_uid',\n 'sha',\n 'source_x',\n 'title',\n 'doi',\n 'pmcid',\n 'pubmed_id',\n 'license',\n 'abstract',\n 'publish_time',\n 'authors',\n 'journal',\n 'mag_id',\n 'who_covidence_id',\n 'arxiv_id',\n 'pdf_json_files',\n 'pmc_json_files',\n 'url',\n 's2_id',\n 'lang',\n 'finished_unigrams',\n 'finished_ngrams',\n 'finished_pos',\n 'finished_pos_ngrams',\n 'filtered_unigrams',\n 'filtered_ngrams',\n 'final',\n 'tf_features',\n 'tf_idf_features',\n 'topicDistribution',\n 'document',\n 'bert_sentence_embeddings',\n 'coSim_topic',\n 'coSim_bert',\n 'coSim_sum']"},"metadata":{}}]},{"cell_type":"code","source":"res_df = cos_df.sort(col(\"coSim_sum\").desc())\\\n                .select(['doi',\"title\",\"url\",\"coSim_topic\",\"coSim_bert\",\"coSim_sum\"])\\\n                .limit(5)","metadata":{"trusted":true},"execution_count":86,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: pop from an empty deque","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-86-810e25c6d7fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcos_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"coSim_sum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doi'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"coSim_topic\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"coSim_bert\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"coSim_sum\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mcol\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mbased\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_invoke_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0mwraps\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mjf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_get_jvm_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m_get_get_jvm_function\u001b[0;34m(name, sc)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mJava\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0massociated\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \"\"\"\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m             \"\\n\" + proto.END_COMMAND_PART)\n\u001b[0m\u001b[1;32m   1696\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_PACKAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mJavaPackage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjvm_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1029\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \"\"\"\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    983\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    984\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:37017)"],"ename":"Py4JNetworkError","evalue":"An error occurred while trying to connect to the Java server (127.0.0.1:37017)","output_type":"error"}]},{"cell_type":"code","source":"fin_df","metadata":{"trusted":true},"execution_count":116,"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"                                   Paper_ID  \\\n0  ae776b55c05bace11b708b29f9281dfc4a98ee4a   \n1  49999396cb9c02d2ec5edb7ef6d4795f3aa850bc   \n2  c5ad3dcba3f39d885de5204d80dddf34e75845c0   \n3  d3b433d903791c682135be571dbdf7835fe56741   \n4  e1e00ef1b70e15de6c0f747907de0089a49114cb   \n5  003218fcd130c4115f9775d3e4412b1c8a1215aa   \n6  5f539f363bd2dfc53c79612f60a8a77cab9402dd   \n7  8a6f529c068f4496811ff60cf1cb9e8783e3b5db   \n8  8cb3c7f7716217bd54a97f220f73dc07ef1220c4   \n9  3d581f72704ea37fa3dc0368f20352cb88ece82f   \n\n                                               title     coSim  \n0  Journal Pre-proof The Impact of COVID-19 As A ...       NaN  \n1  Journal Pre-proof Ecological fallacy in COVID-...       NaN  \n2  Journal Pre-proof A phase 2, open label, multi...       NaN  \n3  The impact of sex, gender and pregnancy on 200...  1.000000  \n4  Inhaled anti-infective chemotherapy for respir...  0.971152  \n5  Antibody seroconversion in asymptomatic and sy...  0.939856  \n6  Journal Pre-proof SARS-CoV-2-related mortality...  0.937925  \n7  Abdominal gastrointestinal imaging findings on...  0.937702  \n8  Machine Learning the Phenomenology of COVID-19...  0.913783  \n9  Article Commentary Role of birth companion in ...  0.913742  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Paper_ID</th>\n      <th>title</th>\n      <th>coSim</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ae776b55c05bace11b708b29f9281dfc4a98ee4a</td>\n      <td>Journal Pre-proof The Impact of COVID-19 As A ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49999396cb9c02d2ec5edb7ef6d4795f3aa850bc</td>\n      <td>Journal Pre-proof Ecological fallacy in COVID-...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>c5ad3dcba3f39d885de5204d80dddf34e75845c0</td>\n      <td>Journal Pre-proof A phase 2, open label, multi...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>d3b433d903791c682135be571dbdf7835fe56741</td>\n      <td>The impact of sex, gender and pregnancy on 200...</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>e1e00ef1b70e15de6c0f747907de0089a49114cb</td>\n      <td>Inhaled anti-infective chemotherapy for respir...</td>\n      <td>0.971152</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>003218fcd130c4115f9775d3e4412b1c8a1215aa</td>\n      <td>Antibody seroconversion in asymptomatic and sy...</td>\n      <td>0.939856</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5f539f363bd2dfc53c79612f60a8a77cab9402dd</td>\n      <td>Journal Pre-proof SARS-CoV-2-related mortality...</td>\n      <td>0.937925</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8a6f529c068f4496811ff60cf1cb9e8783e3b5db</td>\n      <td>Abdominal gastrointestinal imaging findings on...</td>\n      <td>0.937702</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8cb3c7f7716217bd54a97f220f73dc07ef1220c4</td>\n      <td>Machine Learning the Phenomenology of COVID-19...</td>\n      <td>0.913783</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3d581f72704ea37fa3dc0368f20352cb88ece82f</td>\n      <td>Article Commentary Role of birth companion in ...</td>\n      <td>0.913742</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.sort(col(\"coSim\").desc()).limit(10).select(\"abstract\")","metadata":{"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"DataFrame[abstract: string]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Text Preprocessing\nBelow are the text preprocessing functions we are using. ","metadata":{}},{"cell_type":"code","source":"def clean_sentence(sen):\n    '''\n    sen: sentence/document to be cleaned\n    returns: cleaned sentence\n    '''\n    clean = [word for word in sen.split(' ') if word.isalnum()] # keep alpha numeric\n    clean = [word.lower() for word in clean] # lower sentence text\n    clean = [word for word in clean if word not in stop_words] # removes stop words\n    clean = [word for word in clean if len(word)>=4] # removes words with length < 4\n    \n    return clean\n\ndef clean_up(document):\n    '''\n    document: takes in the cleaned sentence from the clean sentence and joins them\n    returns: a \"clean\" document. \n    '''\n    \n    clean = [clean_sentence(a) for a in document]\n    joined = [' '.join(a) for a in clean]\n    \n    return joined\n\ndef topic_render(topic,vocab,wordNum):\n    '''\n    topic: the topic we are looking at\n    vocab: the vocabulary of the corpus\n    wordNum: the number of words in a topic for each Vocab\n    '''\n    \n    terms = topic[1]\n    result = []\n    for i in range(wordNum):\n        term = vocab[terms[i]]\n        result.append(term)\n    return result","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"## main\n\nN = 1000 # number of documents you want to work with. \npaper_path = \"../input/CORD-19-research-challenge/document_parses/pdf_json\"\n\nstart = time()\n#json_files = read_json_files(root_path, spark, N)\n#data = get_body_text(json_files, spark)\ndata = read_paper_data(paper_path, N)\nprint('Done data reading...')\n\n#clean data - \nword_clean_F = F.udf(lambda x: clean_up(x), StringType())\ndata = data.withColumn('clean_body_text',word_clean_F('body'))\ndata = data.select('clean_body_text')\nprint('done cleaning data ...')\n\n#tokenzing - \ntokenizer = Tokenizer(inputCol='clean_body_text',outputCol='tokenized_words')\ntoken_df = tokenizer.transform(data).select('tokenized_words')\nprint('Tokenizing done ...')\n\n# Stop word remover -\nrem = StopWordsRemover(inputCol='tokenized_words',outputCol='filtered') # to ensure all stopwords are removed\nclean_df = rem.transform(token_df).select('filtered')\nprint('Stop words removed ...')\n\n# Count Vectorizer - \ncv = CountVectorizer(inputCol='filtered',outputCol='features')\ncvmodel = cv.fit(clean_df)\ncount_df = cvmodel.transform(clean_df).select('features')\nprint('Done Count Vectorizer ..')\n\n# TF-IDF -\ntfidf = IDF(inputCol='features',outputCol='tf_features')\ntfidfmodel = tfidf.fit(count_df)\ntfidf_df = tfidfmodel.transform(count_df).select('tf_features')\nprint('Done TF-IDF')\nend = time()\n\nprint('total preprocessing time: ',end-start,' seconds')","metadata":{"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Done data reading...\ndone cleaning data ...\nTokenizing done ...\nStop words removed ...\nDone Count Vectorizer ..\nDone TF-IDF\ntotal preprocessing time:  294.3796033859253  seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Latent Dirichlet Allocation","metadata":{}},{"cell_type":"code","source":"##LDA model  \ntopics = 10\nmax_iter = 10\n\nstart = time()\n\nlda = LDA(optimizer='em', k=topics, maxIter= max_iter)\nlda_model = lda.fit(count_df)\nlda_trans = lda_model.transform(count_df)\n\nend = time()\n\nprint('LDA Complete')\nprint('total modeling time: ',end-start,' seconds')","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"LDA Complete\ntotal modeling time:  391.12254548072815  seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"lda_trans.select('topicDistribution').first()","metadata":{"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Row(topicDistribution=DenseVector([0.0009, 0.0027, 0.0005, 0.0069, 0.9856, 0.0007, 0.0009, 0.0005, 0.0005, 0.0009]))"},"metadata":{}}]},{"cell_type":"code","source":"topicIndices","metadata":{"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"PythonRDD[339] at RDD at PythonRDD.scala:53"},"metadata":{}}]},{"cell_type":"code","source":"# terms per topic - \ntopics = lda_model.topicsMatrix()\nvocabArray = cvmodel.vocabulary\nwordnum = 20\ntopicIndices = lda_model.describeTopics(maxTermsPerTopic=wordnum).rdd.map(tuple)\ntopic_final = topicIndices.map(lambda topic: topic_render(topic,vocabArray,wordnum)).collect()\nfor topic in range(len(topic_final)):\n    print('Topic '+str(topic)+':')\n    print(topic_final[topic])","metadata":{"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-10a3a2fe3829>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwordnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtopicIndices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribeTopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxTermsPerTopic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwordnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtopic_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopicIndices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtopic_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabArray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwordnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Topic '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 335.0 failed 1 times, most recent failure: Lost task 0.0 in stage 335.0 (TID 2672) (b52c0c5e5a4a executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-16-10a3a2fe3829>\", line 6, in <lambda>\n  File \"<ipython-input-10-f0379d160fe9>\", line 34, in topic_render\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-16-10a3a2fe3829>\", line 6, in <lambda>\n  File \"<ipython-input-10-f0379d160fe9>\", line 34, in topic_render\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"],"ename":"Py4JJavaError","evalue":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 335.0 failed 1 times, most recent failure: Lost task 0.0 in stage 335.0 (TID 2672) (b52c0c5e5a4a executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-16-10a3a2fe3829>\", line 6, in <lambda>\n  File \"<ipython-input-10-f0379d160fe9>\", line 34, in topic_render\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-16-10a3a2fe3829>\", line 6, in <lambda>\n  File \"<ipython-input-10-f0379d160fe9>\", line 34, in topic_render\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n","output_type":"error"}]},{"cell_type":"code","source":"# Data Visualization - \ndef format_data_pyldavis(clean_df,cvmodel,lda_trans,lda_model):\n    \n    counts = clean_df.select((explode(clean_df.filtered)).alias('tokens')).groupby('tokens').count()\n    \n    wc = {i['tokens']:i['count'] for i in counts.collect()}\n    \n    wc = [wc[x] for x in cvmodel.vocabulary]\n    \n    data = {'topic_term_dists': np.array(lda_model.topicsMatrix().toArray()).T,\n           'doc_topic_dists': np.array([x.toArray() for x in lda_trans.select(['topicDistribution']).toPandas()['topicDistribution']]),\n           'doc_lengths': [x[0] for x in clean_df.select(size(clean_df.filtered)).collect()][:-1],\n           'vocab': cvmodel.vocabulary,\n           'term_frequency': wc}\n    return data\n\ndata = format_data_pyldavis(clean_df,cvmodel,lda_trans,lda_model)\npy_lda_data = pyLDAvis.prepare(**data)\nfile_name = '/output/data-viz.html'\npyLDAvis.display(py_lda_data)","metadata":{"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-6dc0120cc66a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_data_pyldavis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcvmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlda_trans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpy_lda_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/output/data-viz.html'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_lda_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0mdoc_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m     \u001b[0m_input_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' * '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValidationError\u001b[0m: \n * Length of doc_lengths not equal to the number of rows in doc_topic_dists;both should be equal to the number of documents in the data."],"ename":"ValidationError","evalue":"\n * Length of doc_lengths not equal to the number of rows in doc_topic_dists;both should be equal to the number of documents in the data.","output_type":"error"}]},{"cell_type":"code","source":"data['doc_topic_dists']","metadata":{"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"array([[0.09353419, 0.10238294, 0.10021335, ..., 0.08721742, 0.10384926,\n        0.10203226],\n       [0.09944791, 0.10024082, 0.10005861, ..., 0.09885403, 0.1003377 ,\n        0.10019124],\n       [0.09960017, 0.1001783 , 0.10004579, ..., 0.09918371, 0.10024212,\n        0.10013619],\n       ...,\n       [0.10045108, 0.09982114, 0.09996518, ..., 0.10105516, 0.09970343,\n        0.09982186],\n       [0.1004689 , 0.09980995, 0.09996039, ..., 0.10107858, 0.09969482,\n        0.09981826],\n       [0.1004575 , 0.09980776, 0.09995569, ..., 0.10101719, 0.0997082 ,\n        0.09982934]])"},"metadata":{}}]},{"cell_type":"code","source":"pyLDAvis.save_html(py_lda_data,'viz.html')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(r'viz.html')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}